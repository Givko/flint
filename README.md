# ğŸ”¥ Flint

**Flint** is a lightweight, opinionated Python library for writing safe, clean, and high-performance PySpark code. Just like flint sparks fire, this library helps you ignite better Spark pipelinesâ€”with guardrails, best practices, and developer-friendly patterns.

---

## âœ¨ Why Flint?

Working with PySpark is powerfulâ€”but it's easy to fall into traps like:
- Inefficient joins and UDFs
- Excessive .collect() or .toPandas()
- Inconsistent I/O and schema handling
- Poorly structured streaming jobs

Flint helps by providing:
- ğŸ”„ **Composable transformations** via .transform() (like Pandas' .pipe())
- ğŸ§  **Safe helpers** for joins, I/O, schema validation, and performance tuning
- ğŸ§ª **Testable building blocks** for batch and streaming jobs
- ğŸ§° **Minimal APIs** you can adopt graduallyâ€”no lock-in

---

## ğŸ“¦ Example (Coming Soon)

```python
from flint.transforms import clean_column_names, safe_broadcast_join

df = (
    spark.read.csv("data.csv", header=True)
    .transform(clean_column_names)
    .transform(safe_broadcast_join, lookup_df, on="user_id")
)
```

## ğŸ“ Status

Flint is currently in early development.

If you're interested in contributing ideas or want to follow progress, feel free to â­ï¸ star this repo and watch for updates.

## ğŸ› ï¸ Planned Modules - Might change

* `flint.transforms` â€“ clean, testable DataFrame helpers
* `flint.streaming` â€“ safe wrappers for watermarking, state, event-time
* `flint.io` â€“ schema-aware reads & writes
* `flint.utils` â€“ hashing, UUIDs, logging, partitioning
* `flint.decorators` â€“ logging, performance tracing, and guards

## ğŸ’¡ Philosophy

Flint is:
* ğŸ§¼ **Clean** â€“ encourages idiomatic, readable Spark pipelines
* ğŸ§± **Compositional** â€“ each part is modular and testable
* ğŸ§¯ **Safe** â€“ helps teams avoid costly production mistakes
* ğŸ‘¥ **Team-friendly** â€“ makes onboarding and collaboration easier

## ğŸ”— License

MIT â€“ Open and free to use.
